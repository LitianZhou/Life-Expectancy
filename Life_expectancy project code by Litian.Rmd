---
title: "Life_expectancy project code"
author: "LitianZhou"
date: "04/07/2019"
output: pdf_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r init, message=FALSE, warning=FALSE}
library(boot)
library(pls)
library(leaps)
library(gam)
library(glmnet)

life_exp=read.csv("Life Expectancy Data.csv")
life_exp=na.omit(life_exp)
life_exp=life_exp[2:22] # remove the "country" predictor
attach(life_exp)
```


```{r validation set approach}
set.seed(1)
trainid <- sample(1:nrow(life_exp), nrow(life_exp)/2)
train <- life_exp[trainid,]
test <- life_exp[-trainid,]
```

```{r LOOCV for OLS, message=FALSE, warning=FALSE}
require(boot)
glm.fit = glm(Life.expectancy~., data=life_exp)
loocv.err.OLS=cv.glm(life_exp, glm.fit)$delta[1]
loocv.err.OLS
```

```{r BSS forward and backward selection}
require(leaps)

set.seed(7)
regfit.full=regsubsets(Life.expectancy~., life_exp,nvmax=20)
reg.summary=summary(regfit.full)

plot(reg.summary$adjr2, xlab = "Number of variables", ylab = "Adj-Rsq",type ="l")
which.max(reg.summary$adjr2) # give 15
points(15,reg.summary$adjr2[15], col="red",cex=2,pch=20)

plot(reg.summary$bic, xlab = "Number of variables", ylab = "BIC",type ="l")
which.min(reg.summary$bic) # give 12
points(12,reg.summary$bic[12], col="red",cex=2,pch=20)
coef(regfit.full,12)

regfit.fwd=regsubsets(Life.expectancy~., life_exp, nvmax=20, method = "forward")
coef(regfit.fwd,12)

regfit.bwd=regsubsets(Life.expectancy~., life_exp, nvmax=20, method = "backward")
coef(regfit.bwd,12)

# Using 12 predictors, BSS, forward selection and backward selection all give the identical model
```

```{r CV-model selection: select the best size of model}
require(leaps)

# predict function from chapter 6 labs
predict.regsubsets <- function(object, newdata, id, ...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id=id)
  xvars <- names(coefi)
  mat[,xvars]%*%coefi
}

k=10
set.seed(5)
folds=sample(1:k, nrow(life_exp),replace = TRUE)
cv.errors=matrix(NA,k,19,dimnames=list(NULL,paste(1:19)))

for(j in 1:k){
  best.fit=regsubsets(Life.expectancy~., data=life_exp[folds!=j,], nvmax = 20)
  for(i in 1:19){
    pred=predict(best.fit, life_exp[folds==j,],id=i)
    cv.errors[j,i]=mean((life_exp$Life.expectancy[folds==j]-pred)^2)
  }
}
mean.cv.errors=apply(cv.errors,2,mean)
plot(mean.cv.errors,type="b")
which.min(mean.cv.errors) # give 14, depend on the random seed
points(14,mean.cv.errors[14], col="red",cex=2,pch=20)
mean.cv.errors[14]

#use 14 predictors to fit the model
regfit.cv.select=regsubsets(Life.expectancy~.,data=life_exp, nvmax = 14)
coef(regfit.cv.select, 14)
```


```{r Validation set for OLS, message=FALSE, warning=FALSE}
fit.lm <- lm(Life.expectancy~., data=train)
pred.lm <- predict(fit.lm, test)
(err.lm <- mean((test$Life.expectancy - pred.lm)^2))
#err.lm may inflate the error since it uses validation set approach, which does not use whole data to build the model.
```


```{r ridge}
require(glmnet)
xmat.train <- model.matrix(Life.expectancy~., data=train)[,-1]
xmat.test <- model.matrix(Life.expectancy~., data=test)[,-1]
fit.ridge <- cv.glmnet(xmat.train, train$Life.expectancy, alpha=0)
(lambda <- fit.ridge$lambda.min)  # optimal lambda
pred.ridge <- predict(fit.ridge, s=lambda, newx=xmat.test)
(err.ridge <- mean((test$Life.expectancy - pred.ridge)^2))  # test error
lifetest.avg <- mean(life_exp$Life.expectancy)
#ridge.r2 <- 1 - mean((pred.ridge - xmat.test$Life.expectancy)^2) / mean((lifetest.avg - xmat.test$Life.expectancy)^2)
```


```{r lasso}
xmat.train <- model.matrix(Life.expectancy~., data=train)[,-1]
xmat.test <- model.matrix(Life.expectancy~., data=test)[,-1]
fit.lasso <- cv.glmnet(xmat.train, train$Life.expectancy, alpha=1)
(lambda <- fit.lasso$lambda.min)  # optimal lambda
pred.lasso <- predict(fit.lasso, s=lambda, newx=xmat.test)
(err.lasso <- mean((test$Life.expectancy - pred.lasso)^2))  # test error
coef.lasso <- predict(fit.lasso, type="coefficients", s=lambda)[1:ncol(life_exp),]
coef.lasso[coef.lasso != 0]
length(coef.lasso[coef.lasso != 0])
```


```{r PCR: principle component regression}
require(pls)

set.seed(1)
fit.pcr <- pcr(Life.expectancy~., data=train, scale=TRUE, validation="CV")
validationplot(fit.pcr, val.type="MSEP") # I choose M = 7
summary(fit.pcr)
pred.pcr <- predict(fit.pcr, test, ncomp=10)  
(err.pcr <- mean((test$Life.expectancy - pred.pcr)^2))  # test error
```

```{r PLS: partial least square}
set.seed(1)
fit.pls <- plsr(Life.expectancy~., data=train, scale=TRUE, validation="CV")
validationplot(fit.pls, val.type="MSEP")
summary(fit.pls)
pred.pls <- predict(fit.pls, test, ncomp=5)  # min Cv at M=5
(err.pls <- mean((test$Life.expectancy - pred.pls)^2))  # test error
```

## fit the CV_chosen model into poly, regression spline and GAM:
```{r poly, message=FALSE, warning=FALSE}
poly.fit=lm(Life.expectancy~poly(percentage.expenditure,2))
summary(poly.fit)

percentage.expenditurelims = range(percentage.expenditure)
percentage.expenditure.grid=seq(from=percentage.expenditurelims[1], to=percentage.expenditurelims[2])
preds=predict(poly.fit,newdata = list(percentage.expenditure=percentage.expenditure.grid), se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)

par(oma=c(0,0,3,0))
plot(percentage.expenditure,Life.expectancy, xlim=percentage.expenditurelims, col="darkgrey")
title("Degree-2 Polynomial",outer=T)
lines(percentage.expenditure.grid, preds$fit, lwd="2",col="blue")
matlines(percentage.expenditure.grid, se.bands, lwd=1, col="blue",lty=3)
```
The graph shows that as people increase there expenditure on health, the life expenctancy first increase, then decrease (or unchange).

```{r GAM_smoothing_spline}
require(gam)
# Choose from best subset model of 14 predictors. Then remove "Adult.Mortality","infant.deaths" , "under.five.deaths"  and "percentage.expenditure" which are highly skewed. 
gam10.fit=gam(Life.expectancy~Schooling+Year+Status+s(BMI)+ s(Alcohol)+Total.expenditure+Diphtheria+s(HIV.AIDS)+ s(thinness.5.9.years)+Income.composition.of.resources)
par(mfrow=c(2,5))
par(oma=c(0,0,3,0))
plot.Gam(gam10.fit, se=TRUE, col="blue")
title("GAM: linear + smoothing spline fit", outer=TRUE)
gam1.fit=gam(Life.expectancy~Schooling)
gam2.fit=gam(Life.expectancy~Schooling+Year)
gam3.fit=gam(Life.expectancy~Schooling+Year+Status)
gam4.fit=gam(Life.expectancy~Schooling+Year+Status+s(BMI))
gam5.fit=gam(Life.expectancy~Schooling+Year+Status+s(BMI)+s(Alcohol))
gam6.fit=gam(Life.expectancy~Schooling+Year+Status+s(BMI)+s(Alcohol)+Total.expenditure)
gam7.fit=gam(Life.expectancy~Schooling+Year+Status+s(BMI)+s(Alcohol)+Total.expenditure+Diphtheria)
gam8.fit=gam(Life.expectancy~Schooling+Year+Status+s(BMI)+s(Alcohol)+Total.expenditure+Diphtheria+s(HIV.AIDS))
gam9.fit=gam(Life.expectancy~Schooling+Year+Status+s(BMI)+s(Alcohol)+Total.expenditure+Diphtheria+s(HIV.AIDS)+s(thinness.5.9.years))

anova(gam1.fit, gam2.fit, gam3.fit, gam4.fit, gam5.fit, gam6.fit,gam7.fit,gam8.fit,gam9.fit,gam10.fit)
```

```{r validation set approach for GAM}
set.seed(1)
trainid <- sample(1:nrow(life_exp), nrow(life_exp)/2)
train <- life_exp[trainid,]
test <- life_exp[-trainid,]

#according to the ANOVA test, we find the gam6 has a low p-value, to keep the model easier to grasp, we remove "Total.expenditure"

# get the test error by VS approach
gam.fit.train=gam(Life.expectancy~Schooling+Year+Status+s(BMI)+ s(Alcohol)+Diphtheria+s(HIV.AIDS)+ s(thinness.5.9.years)+Income.composition.of.resources, data=train)
gam.final.pred = predict(gam.fit.train, test)
(gam.error = mean((test$Life.expectancy - gam.final.pred)^2))

# fit the model with 9 predictors:
gam.final.fit=gam(Life.expectancy~Schooling+Year+Status+s(BMI)+ s(Alcohol)+Diphtheria+s(HIV.AIDS)+ s(thinness.5.9.years)+Income.composition.of.resources)
summary(gam.final.fit)
```

The MSE of those methods are pretty close,while GAM has the best performace. 
```{r}
err.all <- c(err.lm, loocv.err.OLS, err.ridge, err.lasso, err.pcr, err.pls, gam.error)
names(err.all) <- c("OLS-VS", "OLS-LOOCV", "ridge", "lasso", "pcr", "pls","gam-VS")
barplot(err.all )
```